{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8572480c",
   "metadata": {},
   "source": [
    "mission0と1の対戦、3*3、マイナスなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03f1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magic import *\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4e634",
   "metadata": {},
   "source": [
    "状態と行動を入力に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b6ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_competition(arr: list, i: int):\n",
    "    \"\"\"リストのi個目の要素が何番目に大きいか\"\"\"\n",
    "    v = arr[i]\n",
    "    return sum(x > v for x in arr)\n",
    "\n",
    "def rank_inverse(arr: list, rank: int):\n",
    "    \"\"\"リストでrank番目に大きいのは何個目か\"\"\"\n",
    "    sorted_arr = sorted(arr, reverse=True)\n",
    "    hand = sorted_arr[rank]\n",
    "    for num_index, h in enumerate(arr):\n",
    "        if hand == h: \n",
    "            return num_index\n",
    "\n",
    "# ==== 行動をidと紐づけ ====\n",
    "def id_to_action(size: int, hands: list[int], aid: int):\n",
    "    x = (aid // 4) // size\n",
    "    y = (aid // 4) % size\n",
    "    number = aid % 4 + 1\n",
    "    for num_index, hand in enumerate(hands):\n",
    "        if number == hand:\n",
    "            return x, y, num_index\n",
    "    return x, y, None\n",
    "\n",
    "\n",
    "def make_valid_filter(hands: torch.Tensor, action_space: int, nums_per_cell: int = 4,\n",
    "                      neg_large: float = -1e9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    hands: (N, 3)  各行は {1,2,3,4} から3枚\n",
    "    戻り値: (N, action_space) のフィルター\n",
    "            合法: 0、 非合法: neg_large（例: -1e9）\n",
    "    \"\"\"\n",
    "    device = hands.device\n",
    "    N = hands.size(0)\n",
    "\n",
    "    # 各 action_id の「数字」（1..4）を求める: num[a] = (a % 4) + 1\n",
    "    a = torch.arange(action_space, device=device)\n",
    "    num = (a % nums_per_cell) + 1                      # (36,)\n",
    "\n",
    "    # hands に num が含まれるかを各サンプル・各アクションで判定\n",
    "    # hands: (N,3) → (N,3,1), num: (36,) → (1,1,36)\n",
    "    has_num = (hands.unsqueeze(2) == num.view(1, 1, -1)).any(dim=1)  # (N,36) bool\n",
    "\n",
    "    # 合法: 0、非合法: neg_large\n",
    "    filt = torch.zeros((N, action_space), dtype=torch.float32, device=device)\n",
    "    filt = torch.where(has_num, filt, torch.full_like(filt, neg_large))\n",
    "    return filt\n",
    "\n",
    "\n",
    "def make_move_from_aid(s:GameState, pid: int, aid: int):\n",
    "    x,y,num_index = id_to_action(s.rules.board_size, s.hands[pid], aid)\n",
    "    if not num_index:\n",
    "        return False, None\n",
    "    return make_move(s, pid, x, y, num_index, \"add\")\n",
    "\n",
    "def make_board_tensor(board, device=None, dtype=torch.float32):\n",
    "    # Pythonの処理でNone判定→テンソル化\n",
    "    vals = [[(0 if x is None else x) for x in row] for row in board]\n",
    "    mask = [[(0 if x is None else 1) for x in row] for row in board]\n",
    "    vals_t = torch.tensor(vals, dtype=dtype, device=device)  # (n,n)\n",
    "    mask_t = torch.tensor(mask, dtype=dtype, device=device)  # (n,n)\n",
    "    out = torch.stack([vals_t, mask_t], dim=0)               # (2,n,n)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21579850",
   "metadata": {},
   "source": [
    "報酬を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c33f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(s: GameState):\n",
    "    \"\"\"\n",
    "    戻り値:\n",
    "     done,rewards\n",
    "    勝利ならr=1-round/100、敗北ならr=-1\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    rs = {0:0, 1:0}\n",
    "    round = s.round\n",
    "    for pidx in range(len(s.players)):\n",
    "        if is_victory(s.board, s.missions[pidx]):\n",
    "            rs[pidx] += 1 - round/100\n",
    "            rs[1-pidx] += -1\n",
    "            done = True\n",
    "    \n",
    "    if not done:\n",
    "        if round > 49:\n",
    "            done = True\n",
    "    \n",
    "    return done, rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc14c8",
   "metadata": {},
   "source": [
    "DQNエージェント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50af77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, s: GameState, aid: int, pid: int, reward, next_s: GameState, done: bool):\n",
    "        \n",
    "        board = make_board_tensor(s.board)\n",
    "        hands = torch.tensor(s.hands[pid], dtype = torch.float32)\n",
    "        aid = torch.tensor([aid])\n",
    "        reward = torch.tensor([reward])\n",
    "        next_board = make_board_tensor(next_s.board)\n",
    "        next_hands = torch.tensor(next_s.hands[pid], dtype = torch.float32)\n",
    "        done = torch.tensor([done], dtype=int)\n",
    "        data = (board, hands, aid, reward, next_board, next_hands, done)\n",
    "        self.buffer.append(data)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"\n",
    "        boards, hands, aids, rewards, next_boards, next_hands, dones\n",
    "        \"\"\"\n",
    "        data = random.sample(self.buffer, self.batch_size)\n",
    "        boards, hands, aids, rewards, next_boards, next_hands, dones = map(torch.stack, zip(*data))\n",
    "        return boards, hands, aids.squeeze(), rewards.squeeze(), next_boards, next_hands, dones.squeeze()\n",
    "    #state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qnet_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    boardをCNNで処理、のちに手札と合わせる\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space: int, board_size: int, hand_dim: int = 3):\n",
    "        super().__init__()\n",
    "        self.action_space = action_space\n",
    "        self.board_tower = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),  # → 64*size**2\n",
    "        )\n",
    "        self.hand_tower = nn.Sequential(\n",
    "            nn.Linear(hand_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 32), nn.ReLU(),\n",
    "        )\n",
    "        fused_dim = 32*board_size**2 + 32\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, action_space),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            last = self.head[-1]          # 最後の Linear\n",
    "            last.weight.zero_()\n",
    "            last.bias.zero_()\n",
    "\n",
    "    def forward(self, boards, hands):\n",
    "        \"\"\"\n",
    "        boards (ex: s.board.unsqueeze(0))     : (N, 2, size, size)\n",
    "        hands  (ex: s.hands[pid].unsqueeze(0)): (N, 3)\n",
    "        \"\"\"\n",
    "        hands, _ = torch.sort(hands, dim=1)\n",
    "\n",
    "        hb = self.board_tower(boards)\n",
    "        hh = self.hand_tower(hands)\n",
    "        h = torch.cat([hb, hh], dim=1)\n",
    "        filt = make_valid_filter(hands, self.action_space)\n",
    "        return self.head(h) + filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4f89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class DQNAgent():\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.01\n",
    "        self.epsilon = 0.1\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 64\n",
    "        self.warmup_size = 1000\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        self.board_size = size\n",
    "        self.action_space = (self.board_size**2) * 4 \n",
    "\n",
    "        self.qnet = qnet_CNN(self.action_space, self.board_size)\n",
    "        self.qnet.train()\n",
    "        self.lossfun = nn.SmoothL1Loss() \n",
    "        self.optimizer = optim.SGD(self.qnet.parameters(), self.lr)\n",
    "\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)        \n",
    "\n",
    "    def get_action(self, s: GameState, pid: int, rand: bool = True):\n",
    "        if (np.random.rand() < self.epsilon and rand) or (len(self.replay_buffer) < self.warmup_size):\n",
    "            num = None\n",
    "            count = 0\n",
    "            while num == None:\n",
    "                count += 1\n",
    "                aid = random.randint(0, self.action_space - 1)\n",
    "                _,_,num = id_to_action(self.board_size, s.hands[pid], aid)\n",
    "                if count == 100:\n",
    "                    assert()\n",
    "            return aid\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = self.qnet(\n",
    "                    make_board_tensor(s.board).unsqueeze(0),\n",
    "                    torch.tensor(s.hands[pid], dtype=torch.float32).unsqueeze(0)\n",
    "                    )\n",
    "            best_index = np.argmax(q).item()\n",
    "            return best_index\n",
    "\n",
    "    def update(self, s: GameState, next_s: GameState, aid: int, pid: int, reward: float, done):\n",
    "        if len(self.replay_buffer) < self.warmup_size:\n",
    "            if reward != 0:\n",
    "                self.replay_buffer.add(s, aid, pid, reward, next_s, done)\n",
    "            return\n",
    "        self.replay_buffer.add(s, aid, pid, reward, next_s, done)\n",
    "\n",
    "\n",
    "        boards, hands, aids, rewards, next_boards, next_hands, dones = self.replay_buffer.get_batch()\n",
    "        qs = self.qnet(boards, hands)\n",
    "        \n",
    "        q = qs[np.arange(self.batch_size), aids]\n",
    "\n",
    "        next_qs = torch.tensor(np.zeros([self.batch_size, self.action_space]))\n",
    "        for i in range(5):\n",
    "            new_hand = max(1, i)\n",
    "            next_hands[:,2] = new_hand\n",
    "            next_qs += self.qnet_target(next_boards, next_hands)/5\n",
    "        next_q = next_qs.max(axis=1)\n",
    "\n",
    "        target = rewards + (1 - dones) * self.gamma * next_q.values\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = self.lossfun(q, target)     \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def sync_qnet(self):\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3737ba3",
   "metadata": {},
   "source": [
    "DQN学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a997542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNlearning_btw_agents:\n",
    "    def __init__(self, rules:Rules = Rules()):\n",
    "        self.rules = rules\n",
    "        self.episode = 0\n",
    "        self.episode_buffer = []\n",
    "        self.rewards_0 = []\n",
    "        self.rewards_1 = []\n",
    "\n",
    "    def learning(self, agents: list[DQNAgent], episodes:int, sync_interval: int):\n",
    "        while self.episode < episodes:\n",
    "            s = start_game([\"agent_0\",\"agent1\"],self.rules,mission_id=[0,1])\n",
    "            done = False\n",
    "            rews = deque(maxlen=2)\n",
    "            states = [None, None]\n",
    "            aids = [None, None]\n",
    "            pid = s.turn\n",
    "\n",
    "            while not done:\n",
    "                states[pid] = copy.deepcopy(s)\n",
    "                aid = agents[pid].get_action(s,pid)\n",
    "                aids[pid] = aid\n",
    "                x, y, num_index = id_to_action(3, s.hands[pid], aid)\n",
    "                make_move(s, pid, x, y, num_index,\"add\")\n",
    "\n",
    "                pid = s.turn\n",
    "                done, rew = get_reward(s)\n",
    "                rews.append(rew)\n",
    "                if s.round == 0:\n",
    "                    continue\n",
    "                reward = 0\n",
    "                \"\"\"総報酬を計算し、Qを更新\"\"\"\n",
    "                for rew in rews:\n",
    "                    reward += rew[pid]\n",
    "                agents[pid].update(states[pid], s, aids[pid], pid, reward, done)\n",
    "            \n",
    "            pid = 1 - pid\n",
    "            reward = rews[1][pid]\n",
    "            agents[pid].update(states[pid], s, aids[pid], pid, reward, done)\n",
    "            \n",
    "            is_ready_random = False\n",
    "            if (self.episode + 1) % sync_interval == 0:\n",
    "                print(f\"episode{self.episode} completed\")\n",
    "                for agent in agents:\n",
    "                    agent.sync_qnet()\n",
    "                    if len(agent.replay_buffer) > agent.warmup_size:\n",
    "                        is_ready_random = True\n",
    "                if is_ready_random: \n",
    "                    \"\"\"ランダムと対戦\"\"\"\n",
    "                    self.vs_random(agents)\n",
    "            \n",
    "            self.episode += 1\n",
    "\n",
    "    def vs_random(self, agents: list[DQNAgent]):\n",
    "        reward_0 = 0\n",
    "        reward_1 = 0\n",
    "        action_space = agents[0].action_space\n",
    "        board_size = agents[0].board_size\n",
    "        for game in range(100):\n",
    "            s = start_game([\"agent_0\", \"random\"], self.rules, mission_id=[0,1])\n",
    "            done = False\n",
    "            while not done:\n",
    "                pid = s.turn\n",
    "                if pid == 0:\n",
    "                    aid = agents[pid].get_action(s,pid,False)\n",
    "                    x, y, num_index = id_to_action(3, s.hands[pid], aid)\n",
    "                    make_move(s, pid, x, y, num_index,\"add\")                \n",
    "                else:\n",
    "                    \"\"\"ランダムの操作\"\"\"\n",
    "                    num = None\n",
    "                    while num == None:\n",
    "                        aid = random.randint(0, action_space - 1)\n",
    "                        x,y,num = id_to_action(board_size, s.hands[pid], aid)\n",
    "                    make_move(s, pid, x, y, num,\"add\")\n",
    "                done, rew = get_reward(s)\n",
    "            reward_0 += rew[0]\n",
    "            \n",
    "            s = start_game([\"random\", \"agent_1\"], self.rules, mission_id=[0,1])\n",
    "            done = False\n",
    "            while not done:\n",
    "                pid = s.turn\n",
    "                if pid == 1:\n",
    "                    aid = agents[pid].get_action(s,pid,False)\n",
    "                    x, y, num_index = id_to_action(3, s.hands[pid], aid)\n",
    "                    make_move(s, pid, x, y, num_index,\"add\")                \n",
    "                else:\n",
    "                    \"\"\"ランダムの操作\"\"\"\n",
    "                    num = None\n",
    "                    while num == None:\n",
    "                        aid = random.randint(0, action_space - 1)\n",
    "                        x,y,num = id_to_action(board_size, s.hands[pid], aid)\n",
    "                    make_move(s, pid, x, y, num,\"add\")\n",
    "                done, rew = get_reward(s)\n",
    "            reward_1 += rew[1]\n",
    "        print(f\"reward_0 = {reward_0}\")\n",
    "        print(f\"reward_1 = {reward_1}\")\n",
    "        self.rewards_0.append(reward_0)\n",
    "        self.rewards_1.append(reward_1)\n",
    "        self.episode_buffer.append(self.episode)  \n",
    "    \"\"\"\n",
    "    def graph(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.episode, self.rewards_0, label=\"rewards_1\")   # 1本目\n",
    "        plt.plot(self.episode, self.rewards_1, label=\"rewards_2\")   # 2本目\n",
    "        plt.xlabel(\"episode\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ed8aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_CNN2 = [DQNAgent(size=3), DQNAgent(size=3)]\n",
    "dqnlearning = DQNlearning_btw_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0400c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode5049 completed\n",
      "reward_0 = 3.2399999999999975\n",
      "reward_1 = 8.53\n",
      "episode5099 completed\n",
      "reward_0 = 17.25\n",
      "reward_1 = 15.79\n",
      "episode5149 completed\n",
      "reward_0 = -1.83\n",
      "reward_1 = 37.81999999999999\n",
      "episode5199 completed\n",
      "reward_0 = 14.05\n",
      "reward_1 = 18.000000000000004\n",
      "episode5249 completed\n",
      "reward_0 = 22.55999999999999\n",
      "reward_1 = 8.889999999999997\n",
      "episode5299 completed\n",
      "reward_0 = 31.330000000000005\n",
      "reward_1 = 13.55999999999999\n",
      "episode5349 completed\n",
      "reward_0 = 26.01\n",
      "reward_1 = 11.149999999999999\n",
      "episode5399 completed\n",
      "reward_0 = 10.549999999999999\n",
      "reward_1 = 9.540000000000003\n",
      "episode5449 completed\n",
      "reward_0 = 16.890000000000004\n",
      "reward_1 = 13.459999999999997\n",
      "episode5499 completed\n",
      "reward_0 = 12.91\n",
      "reward_1 = 4.5399999999999965\n",
      "episode5549 completed\n",
      "reward_0 = -10.780000000000001\n",
      "reward_1 = 9.349999999999998\n",
      "episode5599 completed\n",
      "reward_0 = 12.509999999999996\n",
      "reward_1 = 5.209999999999996\n",
      "episode5649 completed\n",
      "reward_0 = 22.090000000000007\n",
      "reward_1 = 31.640000000000008\n",
      "episode5699 completed\n",
      "reward_0 = 11.680000000000005\n",
      "reward_1 = 12.510000000000002\n",
      "episode5749 completed\n",
      "reward_0 = 3.8599999999999994\n",
      "reward_1 = 9.759999999999998\n",
      "episode5799 completed\n",
      "reward_0 = 19.98\n",
      "reward_1 = 9.229999999999999\n",
      "episode5849 completed\n",
      "reward_0 = 12.58\n",
      "reward_1 = 20.680000000000003\n",
      "episode5899 completed\n",
      "reward_0 = 4.839999999999995\n",
      "reward_1 = 1.639999999999997\n",
      "episode5949 completed\n",
      "reward_0 = 17.989999999999995\n",
      "reward_1 = 8.599999999999998\n",
      "episode5999 completed\n",
      "reward_0 = 13.929999999999994\n",
      "reward_1 = 24.290000000000003\n",
      "episode6049 completed\n",
      "reward_0 = 7.399999999999999\n",
      "reward_1 = 5.139999999999999\n",
      "episode6099 completed\n",
      "reward_0 = 3.999999999999999\n",
      "reward_1 = 13.920000000000002\n",
      "episode6149 completed\n",
      "reward_0 = 18.659999999999997\n",
      "reward_1 = 20.299999999999997\n",
      "episode6199 completed\n",
      "reward_0 = 18.840000000000003\n",
      "reward_1 = 24.979999999999997\n",
      "episode6249 completed\n",
      "reward_0 = 30.370000000000005\n",
      "reward_1 = 22.179999999999993\n",
      "episode6299 completed\n",
      "reward_0 = 18.51\n",
      "reward_1 = 5.509999999999997\n",
      "episode6349 completed\n",
      "reward_0 = 14.18\n",
      "reward_1 = 15.389999999999997\n",
      "episode6399 completed\n",
      "reward_0 = 26.15\n",
      "reward_1 = 20.330000000000002\n",
      "episode6449 completed\n",
      "reward_0 = 33.11000000000001\n",
      "reward_1 = 20.159999999999993\n",
      "episode6499 completed\n",
      "reward_0 = 26.869999999999997\n",
      "reward_1 = 11.499999999999996\n",
      "episode6549 completed\n",
      "reward_0 = 25.17\n",
      "reward_1 = 14.45\n",
      "episode6599 completed\n",
      "reward_0 = 22.91\n",
      "reward_1 = 12.869999999999996\n",
      "episode6649 completed\n",
      "reward_0 = 23.839999999999996\n",
      "reward_1 = 7.579999999999997\n",
      "episode6699 completed\n",
      "reward_0 = 14.029999999999998\n",
      "reward_1 = 10.249999999999993\n",
      "episode6749 completed\n",
      "reward_0 = 27.19\n",
      "reward_1 = 25.769999999999992\n",
      "episode6799 completed\n",
      "reward_0 = 17.470000000000002\n",
      "reward_1 = 16.679999999999996\n",
      "episode6849 completed\n",
      "reward_0 = 30.019999999999992\n",
      "reward_1 = 23.88\n",
      "episode6899 completed\n",
      "reward_0 = 9.990000000000006\n",
      "reward_1 = 25.390000000000008\n",
      "episode6949 completed\n",
      "reward_0 = 8.02\n",
      "reward_1 = 18.2\n",
      "episode6999 completed\n",
      "reward_0 = 21.71\n",
      "reward_1 = 17.630000000000003\n",
      "episode7049 completed\n",
      "reward_0 = 18.7\n",
      "reward_1 = 1.5599999999999987\n",
      "episode7099 completed\n",
      "reward_0 = 13.929999999999994\n",
      "reward_1 = 23.680000000000003\n",
      "episode7149 completed\n",
      "reward_0 = 21.939999999999994\n",
      "reward_1 = 4.819999999999999\n",
      "episode7199 completed\n",
      "reward_0 = 23.91\n",
      "reward_1 = 16.639999999999997\n",
      "episode7249 completed\n",
      "reward_0 = 13.539999999999997\n",
      "reward_1 = 20.22\n",
      "episode7299 completed\n",
      "reward_0 = 28.6\n",
      "reward_1 = 7.030000000000001\n",
      "episode7349 completed\n",
      "reward_0 = 16.7\n",
      "reward_1 = 6.939999999999996\n",
      "episode7399 completed\n",
      "reward_0 = 26.479999999999993\n",
      "reward_1 = 17.530000000000005\n",
      "episode7449 completed\n",
      "reward_0 = 23.490000000000002\n",
      "reward_1 = 19.070000000000007\n",
      "episode7499 completed\n",
      "reward_0 = 24.489999999999995\n",
      "reward_1 = 11.569999999999999\n",
      "episode7549 completed\n",
      "reward_0 = 24.920000000000005\n",
      "reward_1 = 18.709999999999997\n",
      "episode7599 completed\n",
      "reward_0 = 26.669999999999995\n",
      "reward_1 = 6.430000000000001\n",
      "episode7649 completed\n",
      "reward_0 = 26.379999999999992\n",
      "reward_1 = 14.470000000000002\n",
      "episode7699 completed\n",
      "reward_0 = 26.1\n",
      "reward_1 = 18.330000000000002\n",
      "episode7749 completed\n",
      "reward_0 = 16.829999999999988\n",
      "reward_1 = 22.210000000000008\n",
      "episode7799 completed\n",
      "reward_0 = 25.419999999999998\n",
      "reward_1 = 4.4099999999999975\n",
      "episode7849 completed\n",
      "reward_0 = 30.13\n",
      "reward_1 = 22.15\n",
      "episode7899 completed\n",
      "reward_0 = 17.28000000000001\n",
      "reward_1 = 19.19\n",
      "episode7949 completed\n",
      "reward_0 = 23.110000000000014\n",
      "reward_1 = 15.490000000000002\n",
      "episode7999 completed\n",
      "reward_0 = 20.30999999999999\n",
      "reward_1 = 20.29999999999999\n",
      "episode8049 completed\n",
      "reward_0 = 35.16\n",
      "reward_1 = 8.79\n",
      "episode8099 completed\n",
      "reward_0 = 19.81000000000001\n",
      "reward_1 = 21.43\n",
      "episode8149 completed\n",
      "reward_0 = 23.91\n",
      "reward_1 = 13.77\n",
      "episode8199 completed\n",
      "reward_0 = 29.789999999999996\n",
      "reward_1 = 22.269999999999996\n",
      "episode8249 completed\n",
      "reward_0 = 21.68\n",
      "reward_1 = 15.589999999999995\n",
      "episode8299 completed\n",
      "reward_0 = 28.540000000000003\n",
      "reward_1 = 36.11\n",
      "episode8349 completed\n",
      "reward_0 = 41.98\n",
      "reward_1 = 14.650000000000007\n",
      "episode8399 completed\n",
      "reward_0 = 30.260000000000005\n",
      "reward_1 = 28.580000000000002\n",
      "episode8449 completed\n",
      "reward_0 = 26.86\n",
      "reward_1 = 39.530000000000015\n",
      "episode8499 completed\n",
      "reward_0 = 35.99\n",
      "reward_1 = 12.359999999999994\n",
      "episode8549 completed\n",
      "reward_0 = 18.11\n",
      "reward_1 = 18.409999999999993\n",
      "episode8599 completed\n",
      "reward_0 = 40.10999999999999\n",
      "reward_1 = 14.950000000000001\n",
      "episode8649 completed\n",
      "reward_0 = 33.730000000000004\n",
      "reward_1 = 19.9\n",
      "episode8699 completed\n",
      "reward_0 = 19.779999999999998\n",
      "reward_1 = 10.72\n",
      "episode8749 completed\n",
      "reward_0 = 42.870000000000005\n",
      "reward_1 = 31.509999999999998\n",
      "episode8799 completed\n",
      "reward_0 = 19.37000000000001\n",
      "reward_1 = 31.2\n",
      "episode8849 completed\n",
      "reward_0 = 32.2\n",
      "reward_1 = 3.2399999999999984\n",
      "episode8899 completed\n",
      "reward_0 = 30.019999999999996\n",
      "reward_1 = 22.559999999999995\n",
      "episode8949 completed\n",
      "reward_0 = 35.91999999999999\n",
      "reward_1 = 13.559999999999999\n",
      "episode8999 completed\n",
      "reward_0 = 28.55\n",
      "reward_1 = 20.790000000000006\n",
      "episode9049 completed\n",
      "reward_0 = 40.10999999999998\n",
      "reward_1 = 16.619999999999997\n",
      "episode9099 completed\n",
      "reward_0 = 25.03\n",
      "reward_1 = 11.319999999999993\n",
      "episode9149 completed\n",
      "reward_0 = 37.87\n",
      "reward_1 = 13.980000000000004\n",
      "episode9199 completed\n",
      "reward_0 = 25.059999999999995\n",
      "reward_1 = 12.74\n",
      "episode9249 completed\n",
      "reward_0 = 33.16\n",
      "reward_1 = 32.64000000000001\n",
      "episode9299 completed\n",
      "reward_0 = 27.25\n",
      "reward_1 = 31.590000000000003\n",
      "episode9349 completed\n",
      "reward_0 = 25.870000000000005\n",
      "reward_1 = 16.47\n",
      "episode9399 completed\n",
      "reward_0 = 37.56999999999999\n",
      "reward_1 = 42.55999999999998\n",
      "episode9449 completed\n",
      "reward_0 = 29.710000000000004\n",
      "reward_1 = 11.540000000000001\n",
      "episode9499 completed\n",
      "reward_0 = 34.400000000000006\n",
      "reward_1 = 8.89\n",
      "episode9549 completed\n",
      "reward_0 = 34.65999999999999\n",
      "reward_1 = 28.75000000000001\n",
      "episode9599 completed\n",
      "reward_0 = 38.22\n",
      "reward_1 = 34.39\n",
      "episode9649 completed\n",
      "reward_0 = 30.690000000000005\n",
      "reward_1 = 9.429999999999996\n",
      "episode9699 completed\n",
      "reward_0 = 29.470000000000002\n",
      "reward_1 = 31.349999999999994\n",
      "episode9749 completed\n",
      "reward_0 = 34.53999999999999\n",
      "reward_1 = 33.19\n",
      "episode9799 completed\n",
      "reward_0 = 43.58999999999999\n",
      "reward_1 = 33.16000000000001\n",
      "episode9849 completed\n",
      "reward_0 = 22.380000000000006\n",
      "reward_1 = 4.939999999999998\n",
      "episode9899 completed\n",
      "reward_0 = 26.380000000000003\n",
      "reward_1 = 27.83000000000001\n",
      "episode9949 completed\n",
      "reward_0 = 38.14\n",
      "reward_1 = 20.030000000000005\n",
      "episode9999 completed\n",
      "reward_0 = 36.18\n",
      "reward_1 = 7.080000000000001\n",
      "episode10049 completed\n",
      "reward_0 = 26.400000000000006\n",
      "reward_1 = 10.920000000000002\n",
      "episode10099 completed\n",
      "reward_0 = 40.56999999999998\n",
      "reward_1 = 25.79\n",
      "episode10149 completed\n",
      "reward_0 = 14.700000000000003\n",
      "reward_1 = 31.320000000000014\n",
      "episode10199 completed\n",
      "reward_0 = 42.29999999999998\n",
      "reward_1 = 26.709999999999994\n",
      "episode10249 completed\n",
      "reward_0 = 14.399999999999999\n",
      "reward_1 = 8.780000000000003\n",
      "episode10299 completed\n",
      "reward_0 = 37.040000000000006\n",
      "reward_1 = 28.840000000000003\n",
      "episode10349 completed\n",
      "reward_0 = 22.98\n",
      "reward_1 = 18.9\n",
      "episode10399 completed\n",
      "reward_0 = 33.620000000000005\n",
      "reward_1 = 25.31\n",
      "episode10449 completed\n",
      "reward_0 = 38.12999999999999\n",
      "reward_1 = 3.5299999999999985\n",
      "episode10499 completed\n",
      "reward_0 = 23.38\n",
      "reward_1 = 15.240000000000002\n",
      "episode10549 completed\n",
      "reward_0 = 25.83000000000001\n",
      "reward_1 = 17.58\n",
      "episode10599 completed\n",
      "reward_0 = 36.48999999999999\n",
      "reward_1 = 16.450000000000003\n",
      "episode10649 completed\n",
      "reward_0 = 15.120000000000005\n",
      "reward_1 = 9.009999999999996\n",
      "episode10699 completed\n",
      "reward_0 = 21.969999999999995\n",
      "reward_1 = 17.639999999999997\n",
      "episode10749 completed\n",
      "reward_0 = 27.470000000000013\n",
      "reward_1 = 15.830000000000002\n",
      "episode10799 completed\n",
      "reward_0 = 29.440000000000005\n",
      "reward_1 = 17.169999999999998\n",
      "episode10849 completed\n",
      "reward_0 = 17.13\n",
      "reward_1 = 28.019999999999996\n",
      "episode10899 completed\n",
      "reward_0 = 41.23999999999999\n",
      "reward_1 = 32.82\n",
      "episode10949 completed\n",
      "reward_0 = 23.750000000000004\n",
      "reward_1 = 24.400000000000006\n",
      "episode10999 completed\n",
      "reward_0 = 29.669999999999998\n",
      "reward_1 = 29.12\n",
      "episode11049 completed\n",
      "reward_0 = 40.75\n",
      "reward_1 = 12.69\n",
      "episode11099 completed\n",
      "reward_0 = 24.489999999999995\n",
      "reward_1 = 28.15000000000001\n",
      "episode11149 completed\n",
      "reward_0 = 20.329999999999995\n",
      "reward_1 = 19.740000000000002\n",
      "episode11199 completed\n",
      "reward_0 = 36.88000000000001\n",
      "reward_1 = 25.69000000000001\n",
      "episode11249 completed\n",
      "reward_0 = 25.300000000000004\n",
      "reward_1 = 18.440000000000005\n",
      "episode11299 completed\n",
      "reward_0 = 42.64\n",
      "reward_1 = 37.66\n",
      "episode11349 completed\n",
      "reward_0 = 23.4\n",
      "reward_1 = 23.34\n",
      "episode11399 completed\n",
      "reward_0 = 24.909999999999997\n",
      "reward_1 = 26.409999999999997\n",
      "episode11449 completed\n",
      "reward_0 = 19.18\n",
      "reward_1 = 16.65\n",
      "episode11499 completed\n",
      "reward_0 = 36.74999999999999\n",
      "reward_1 = 32.29\n",
      "episode11549 completed\n",
      "reward_0 = 27.779999999999998\n",
      "reward_1 = 24.78000000000001\n",
      "episode11599 completed\n",
      "reward_0 = 31.43000000000002\n",
      "reward_1 = 25.309999999999995\n",
      "episode11649 completed\n",
      "reward_0 = 28.439999999999994\n",
      "reward_1 = 13.360000000000005\n",
      "episode11699 completed\n",
      "reward_0 = 39.66\n",
      "reward_1 = 27.179999999999996\n",
      "episode11749 completed\n",
      "reward_0 = 19.53\n",
      "reward_1 = 25.2\n",
      "episode11799 completed\n",
      "reward_0 = 13.299999999999997\n",
      "reward_1 = 36.059999999999995\n",
      "episode11849 completed\n",
      "reward_0 = 34.53000000000001\n",
      "reward_1 = 25.68\n",
      "episode11899 completed\n",
      "reward_0 = 30.93\n",
      "reward_1 = 24.810000000000006\n",
      "episode11949 completed\n",
      "reward_0 = 25.190000000000012\n",
      "reward_1 = 35.88000000000001\n",
      "episode11999 completed\n",
      "reward_0 = 31.429999999999996\n",
      "reward_1 = 15.430000000000003\n",
      "episode12049 completed\n",
      "reward_0 = 16.470000000000002\n",
      "reward_1 = 28.109999999999996\n",
      "episode12099 completed\n",
      "reward_0 = 33.81\n",
      "reward_1 = 23.840000000000007\n",
      "episode12149 completed\n",
      "reward_0 = 22.250000000000004\n",
      "reward_1 = 11.759999999999998\n",
      "episode12199 completed\n",
      "reward_0 = 25.329999999999995\n",
      "reward_1 = 19.480000000000004\n",
      "episode12249 completed\n",
      "reward_0 = 28.250000000000007\n",
      "reward_1 = 12.240000000000002\n",
      "episode12299 completed\n",
      "reward_0 = 40.080000000000005\n",
      "reward_1 = 32.56999999999999\n",
      "episode12349 completed\n",
      "reward_0 = 26.459999999999997\n",
      "reward_1 = 26.610000000000003\n",
      "episode12399 completed\n",
      "reward_0 = 34.269999999999996\n",
      "reward_1 = 11.379999999999999\n",
      "episode12449 completed\n",
      "reward_0 = 34.23999999999999\n",
      "reward_1 = 27.080000000000002\n",
      "episode12499 completed\n",
      "reward_0 = 39.569999999999986\n",
      "reward_1 = 26.379999999999995\n",
      "episode12549 completed\n",
      "reward_0 = 25.820000000000007\n",
      "reward_1 = 10.729999999999999\n",
      "episode12599 completed\n",
      "reward_0 = 28.560000000000006\n",
      "reward_1 = 15.210000000000004\n",
      "episode12649 completed\n",
      "reward_0 = 23.91\n",
      "reward_1 = 16.690000000000005\n",
      "episode12699 completed\n",
      "reward_0 = 33.60999999999998\n",
      "reward_1 = 32.28000000000001\n",
      "episode12749 completed\n",
      "reward_0 = 25.500000000000018\n",
      "reward_1 = 33.26\n",
      "episode12799 completed\n",
      "reward_0 = 28.57999999999999\n",
      "reward_1 = 8.769999999999998\n",
      "episode12849 completed\n",
      "reward_0 = 41.73\n",
      "reward_1 = 9.149999999999997\n",
      "episode12899 completed\n",
      "reward_0 = 34.25\n",
      "reward_1 = 22.150000000000013\n",
      "episode12949 completed\n",
      "reward_0 = 39.489999999999995\n",
      "reward_1 = 25.169999999999998\n",
      "episode12999 completed\n",
      "reward_0 = 33.76\n",
      "reward_1 = 26.320000000000007\n",
      "episode13049 completed\n",
      "reward_0 = 36.879999999999995\n",
      "reward_1 = 24.480000000000004\n",
      "episode13099 completed\n",
      "reward_0 = 37.34\n",
      "reward_1 = 15.420000000000002\n",
      "episode13149 completed\n",
      "reward_0 = 27.169999999999998\n",
      "reward_1 = 13.240000000000002\n",
      "episode13199 completed\n",
      "reward_0 = 34.38999999999999\n",
      "reward_1 = 27.14\n",
      "episode13249 completed\n",
      "reward_0 = 37.050000000000004\n",
      "reward_1 = 20.019999999999996\n",
      "episode13299 completed\n",
      "reward_0 = 30.210000000000008\n",
      "reward_1 = 32.06999999999999\n",
      "episode13349 completed\n",
      "reward_0 = 38.82\n",
      "reward_1 = 21.680000000000007\n",
      "episode13399 completed\n",
      "reward_0 = 35.97\n",
      "reward_1 = 22.7\n",
      "episode13449 completed\n",
      "reward_0 = 36.42000000000001\n",
      "reward_1 = 5.6299999999999955\n",
      "episode13499 completed\n",
      "reward_0 = 26.05\n",
      "reward_1 = 19.429999999999996\n",
      "episode13549 completed\n",
      "reward_0 = 36.4\n",
      "reward_1 = 18.800000000000004\n",
      "episode13599 completed\n",
      "reward_0 = 37.099999999999994\n",
      "reward_1 = 29.72\n",
      "episode13649 completed\n",
      "reward_0 = 40.61999999999999\n",
      "reward_1 = 20.390000000000008\n",
      "episode13699 completed\n",
      "reward_0 = 46.969999999999985\n",
      "reward_1 = 28.750000000000004\n",
      "episode13749 completed\n",
      "reward_0 = 39.969999999999985\n",
      "reward_1 = 21.98\n",
      "episode13799 completed\n",
      "reward_0 = 29.860000000000003\n",
      "reward_1 = 26.93\n",
      "episode13849 completed\n",
      "reward_0 = 40.71999999999999\n",
      "reward_1 = 13.879999999999999\n",
      "episode13899 completed\n",
      "reward_0 = 19.54999999999999\n",
      "reward_1 = 21.829999999999995\n",
      "episode13949 completed\n",
      "reward_0 = 36.35999999999999\n",
      "reward_1 = 14.810000000000002\n",
      "episode13999 completed\n",
      "reward_0 = 40.46\n",
      "reward_1 = 18.28\n",
      "episode14049 completed\n",
      "reward_0 = 26.720000000000006\n",
      "reward_1 = 32.74999999999999\n",
      "episode14099 completed\n",
      "reward_0 = 29.979999999999997\n",
      "reward_1 = 29.26000000000001\n",
      "episode14149 completed\n",
      "reward_0 = 28.290000000000006\n",
      "reward_1 = 14.519999999999996\n",
      "episode14199 completed\n",
      "reward_0 = 43.699999999999996\n",
      "reward_1 = 21.390000000000008\n",
      "episode14249 completed\n",
      "reward_0 = 36.239999999999995\n",
      "reward_1 = 36.08000000000001\n",
      "episode14299 completed\n",
      "reward_0 = 27.140000000000004\n",
      "reward_1 = 17.280000000000005\n",
      "episode14349 completed\n",
      "reward_0 = 26.85\n",
      "reward_1 = 8.839999999999998\n",
      "episode14399 completed\n",
      "reward_0 = 44.09999999999999\n",
      "reward_1 = 17.360000000000007\n",
      "episode14449 completed\n",
      "reward_0 = 37.40999999999998\n",
      "reward_1 = 20.610000000000003\n",
      "episode14499 completed\n",
      "reward_0 = 38.440000000000005\n",
      "reward_1 = 5.189999999999997\n",
      "episode14549 completed\n",
      "reward_0 = 23.599999999999998\n",
      "reward_1 = 28.660000000000004\n",
      "episode14599 completed\n",
      "reward_0 = 32.690000000000005\n",
      "reward_1 = 16.96\n",
      "episode14649 completed\n",
      "reward_0 = 37.84\n",
      "reward_1 = 28.33\n",
      "episode14699 completed\n",
      "reward_0 = 23.58\n",
      "reward_1 = 40.760000000000005\n",
      "episode14749 completed\n",
      "reward_0 = 39.209999999999994\n",
      "reward_1 = 28.929999999999996\n",
      "episode14799 completed\n",
      "reward_0 = 33.81\n",
      "reward_1 = 14.7\n",
      "episode14849 completed\n",
      "reward_0 = 27.220000000000002\n",
      "reward_1 = 23.009999999999998\n",
      "episode14899 completed\n",
      "reward_0 = 28.820000000000007\n",
      "reward_1 = 26.37999999999999\n",
      "episode14949 completed\n",
      "reward_0 = 21.51\n",
      "reward_1 = 39.95000000000001\n",
      "episode14999 completed\n",
      "reward_0 = 43.51\n",
      "reward_1 = 21.630000000000006\n",
      "episode15049 completed\n",
      "reward_0 = 38.449999999999996\n",
      "reward_1 = 21.339999999999996\n",
      "episode15099 completed\n",
      "reward_0 = 48.909999999999975\n",
      "reward_1 = 14.999999999999998\n",
      "episode15149 completed\n",
      "reward_0 = 33.8\n",
      "reward_1 = 18.13\n",
      "episode15199 completed\n",
      "reward_0 = 22.57\n",
      "reward_1 = 15.91\n",
      "episode15249 completed\n",
      "reward_0 = 23.97\n",
      "reward_1 = 33.959999999999994\n",
      "episode15299 completed\n",
      "reward_0 = 42.209999999999994\n",
      "reward_1 = 17.480000000000004\n",
      "episode15349 completed\n",
      "reward_0 = 45.06999999999999\n",
      "reward_1 = 36.64999999999999\n",
      "episode15399 completed\n",
      "reward_0 = 27.149999999999995\n",
      "reward_1 = 22.57999999999999\n",
      "episode15449 completed\n",
      "reward_0 = 36.33\n",
      "reward_1 = 30.78\n",
      "episode15499 completed\n",
      "reward_0 = 26.739999999999995\n",
      "reward_1 = 22.229999999999997\n",
      "episode15549 completed\n",
      "reward_0 = 36.93999999999998\n",
      "reward_1 = 31.39\n",
      "episode15599 completed\n",
      "reward_0 = 42.36000000000001\n",
      "reward_1 = 28.96\n",
      "episode15649 completed\n",
      "reward_0 = 34.300000000000004\n",
      "reward_1 = 19.659999999999997\n",
      "episode15699 completed\n",
      "reward_0 = 28.580000000000002\n",
      "reward_1 = 17.58000000000001\n",
      "episode15749 completed\n",
      "reward_0 = 28.880000000000013\n",
      "reward_1 = 30.609999999999996\n",
      "episode15799 completed\n",
      "reward_0 = 39.72\n",
      "reward_1 = 32.410000000000004\n",
      "episode15849 completed\n",
      "reward_0 = 30.970000000000002\n",
      "reward_1 = 26.930000000000003\n",
      "episode15899 completed\n",
      "reward_0 = 24.62\n",
      "reward_1 = 15.869999999999997\n",
      "episode15949 completed\n",
      "reward_0 = 45.64999999999997\n",
      "reward_1 = 25.94\n",
      "episode15999 completed\n",
      "reward_0 = 27.860000000000003\n",
      "reward_1 = 29.82\n",
      "episode16049 completed\n",
      "reward_0 = 25.390000000000008\n",
      "reward_1 = 19.739999999999995\n",
      "episode16099 completed\n",
      "reward_0 = 29.279999999999998\n",
      "reward_1 = 24.65999999999999\n",
      "episode16149 completed\n",
      "reward_0 = 31.000000000000007\n",
      "reward_1 = 22.78\n",
      "episode16199 completed\n",
      "reward_0 = 27.64\n",
      "reward_1 = 22.28\n",
      "episode16249 completed\n",
      "reward_0 = 30.55\n",
      "reward_1 = 21.1\n",
      "episode16299 completed\n",
      "reward_0 = 36.06999999999999\n",
      "reward_1 = 22.060000000000006\n",
      "episode16349 completed\n",
      "reward_0 = 34.18000000000001\n",
      "reward_1 = 16.709999999999997\n",
      "episode16399 completed\n",
      "reward_0 = 32.750000000000014\n",
      "reward_1 = 22.530000000000005\n",
      "episode16449 completed\n",
      "reward_0 = 19.319999999999997\n",
      "reward_1 = 32.050000000000004\n",
      "episode16499 completed\n",
      "reward_0 = 39.829999999999984\n",
      "reward_1 = 38.42999999999999\n",
      "episode16549 completed\n",
      "reward_0 = 39.19999999999999\n",
      "reward_1 = 11.360000000000001\n",
      "episode16599 completed\n",
      "reward_0 = 34.3\n",
      "reward_1 = 11.129999999999999\n",
      "episode16649 completed\n",
      "reward_0 = 20.210000000000004\n",
      "reward_1 = 24.02\n",
      "episode16699 completed\n",
      "reward_0 = 36.24000000000001\n",
      "reward_1 = 26.929999999999996\n",
      "episode16749 completed\n",
      "reward_0 = 34.79\n",
      "reward_1 = 37.25\n",
      "episode16799 completed\n",
      "reward_0 = 33.58\n",
      "reward_1 = 19.149999999999995\n",
      "episode16849 completed\n",
      "reward_0 = 39.79\n",
      "reward_1 = 21.810000000000002\n",
      "episode16899 completed\n",
      "reward_0 = 29.520000000000007\n",
      "reward_1 = 33.99\n",
      "episode16949 completed\n",
      "reward_0 = 31.820000000000007\n",
      "reward_1 = 32.6\n",
      "episode16999 completed\n",
      "reward_0 = 38.53999999999999\n",
      "reward_1 = 27.48\n",
      "episode17049 completed\n",
      "reward_0 = 34.42999999999999\n",
      "reward_1 = 30.75\n",
      "episode17099 completed\n",
      "reward_0 = 24.060000000000002\n",
      "reward_1 = 38.72\n",
      "episode17149 completed\n",
      "reward_0 = 40.31999999999999\n",
      "reward_1 = 16.000000000000004\n",
      "episode17199 completed\n",
      "reward_0 = 31.569999999999986\n",
      "reward_1 = 19.95\n",
      "episode17249 completed\n",
      "reward_0 = 51.27999999999997\n",
      "reward_1 = 31.820000000000007\n",
      "episode17299 completed\n",
      "reward_0 = 37.86999999999998\n",
      "reward_1 = 26.30000000000001\n",
      "episode17349 completed\n",
      "reward_0 = 36.66999999999999\n",
      "reward_1 = 23.819999999999997\n",
      "episode17399 completed\n",
      "reward_0 = 39.18999999999998\n",
      "reward_1 = 33.230000000000004\n",
      "episode17449 completed\n",
      "reward_0 = 43.62999999999999\n",
      "reward_1 = 28.16\n",
      "episode17499 completed\n",
      "reward_0 = 35.02999999999999\n",
      "reward_1 = 33.510000000000005\n",
      "episode17549 completed\n",
      "reward_0 = 36.81\n",
      "reward_1 = 26.670000000000005\n",
      "episode17599 completed\n",
      "reward_0 = 50.010000000000005\n",
      "reward_1 = 18.230000000000004\n",
      "episode17649 completed\n",
      "reward_0 = 22.21\n",
      "reward_1 = 28.899999999999995\n",
      "episode17699 completed\n",
      "reward_0 = 28.14\n",
      "reward_1 = 31.13\n",
      "episode17749 completed\n",
      "reward_0 = 26.630000000000003\n",
      "reward_1 = 36.08999999999998\n",
      "episode17799 completed\n",
      "reward_0 = 34.26\n",
      "reward_1 = 31.08000000000001\n",
      "episode17849 completed\n",
      "reward_0 = 31.07\n",
      "reward_1 = 20.62999999999999\n",
      "episode17899 completed\n",
      "reward_0 = 27.550000000000008\n",
      "reward_1 = 25.25\n",
      "episode17949 completed\n",
      "reward_0 = 33.35999999999999\n",
      "reward_1 = 39.019999999999996\n",
      "episode17999 completed\n",
      "reward_0 = 23.98999999999999\n",
      "reward_1 = 31.530000000000012\n",
      "episode18049 completed\n",
      "reward_0 = 30.999999999999996\n",
      "reward_1 = 26.950000000000006\n",
      "episode18099 completed\n",
      "reward_0 = 25.94\n",
      "reward_1 = 18.910000000000004\n",
      "episode18149 completed\n",
      "reward_0 = 35.830000000000005\n",
      "reward_1 = 18.380000000000006\n",
      "episode18199 completed\n",
      "reward_0 = 26.140000000000004\n",
      "reward_1 = 38.07\n",
      "episode18249 completed\n",
      "reward_0 = 25.829999999999995\n",
      "reward_1 = 25.26000000000001\n",
      "episode18299 completed\n",
      "reward_0 = 14.830000000000004\n",
      "reward_1 = 33.55\n",
      "episode18349 completed\n",
      "reward_0 = 27.680000000000003\n",
      "reward_1 = 16.75\n",
      "episode18399 completed\n",
      "reward_0 = 19.35\n",
      "reward_1 = 37.070000000000014\n",
      "episode18449 completed\n",
      "reward_0 = 27.820000000000004\n",
      "reward_1 = 36.85\n",
      "episode18499 completed\n",
      "reward_0 = 43.63999999999999\n",
      "reward_1 = 21.420000000000005\n",
      "episode18549 completed\n",
      "reward_0 = 42.35\n",
      "reward_1 = 27.560000000000006\n",
      "episode18599 completed\n",
      "reward_0 = 22.300000000000004\n",
      "reward_1 = 31.5\n",
      "episode18649 completed\n",
      "reward_0 = 10.27\n",
      "reward_1 = 13.889999999999997\n",
      "episode18699 completed\n",
      "reward_0 = 26.910000000000004\n",
      "reward_1 = 29.18000000000001\n",
      "episode18749 completed\n",
      "reward_0 = 24.710000000000004\n",
      "reward_1 = 27.070000000000004\n",
      "episode18799 completed\n",
      "reward_0 = 22.010000000000005\n",
      "reward_1 = 18.470000000000006\n",
      "episode18849 completed\n",
      "reward_0 = 38.35\n",
      "reward_1 = 23.91\n",
      "episode18899 completed\n",
      "reward_0 = 16.7\n",
      "reward_1 = 19.410000000000004\n",
      "episode18949 completed\n",
      "reward_0 = 39.3\n",
      "reward_1 = 38.25000000000001\n",
      "episode18999 completed\n",
      "reward_0 = 24.35\n",
      "reward_1 = 37.74000000000001\n",
      "episode19049 completed\n",
      "reward_0 = 27.740000000000006\n",
      "reward_1 = 22.77\n",
      "episode19099 completed\n",
      "reward_0 = 29.7\n",
      "reward_1 = 34.49999999999999\n",
      "episode19149 completed\n",
      "reward_0 = 22.71000000000001\n",
      "reward_1 = 40.35\n",
      "episode19199 completed\n",
      "reward_0 = 9.809999999999999\n",
      "reward_1 = 11.590000000000002\n",
      "episode19249 completed\n",
      "reward_0 = 31.09999999999999\n",
      "reward_1 = 31.719999999999988\n",
      "episode19299 completed\n",
      "reward_0 = 35.59\n",
      "reward_1 = 32.220000000000006\n",
      "episode19349 completed\n",
      "reward_0 = 16.970000000000006\n",
      "reward_1 = 32.28999999999999\n",
      "episode19399 completed\n",
      "reward_0 = 29.14\n",
      "reward_1 = 35.64000000000001\n",
      "episode19449 completed\n",
      "reward_0 = 38.66\n",
      "reward_1 = 29.04999999999999\n",
      "episode19499 completed\n",
      "reward_0 = 46.919999999999995\n",
      "reward_1 = 39.59999999999999\n",
      "episode19549 completed\n",
      "reward_0 = 21.890000000000008\n",
      "reward_1 = 31.34\n",
      "episode19599 completed\n",
      "reward_0 = 20.11\n",
      "reward_1 = 27.690000000000005\n",
      "episode19649 completed\n",
      "reward_0 = 29.620000000000005\n",
      "reward_1 = 17.920000000000005\n",
      "episode19699 completed\n",
      "reward_0 = 28.59000000000001\n",
      "reward_1 = 18.780000000000005\n",
      "episode19749 completed\n",
      "reward_0 = 37.28\n",
      "reward_1 = 28.950000000000003\n",
      "episode19799 completed\n",
      "reward_0 = 9.41\n",
      "reward_1 = 26.219999999999995\n",
      "episode19849 completed\n",
      "reward_0 = 45.06\n",
      "reward_1 = 33.190000000000005\n",
      "episode19899 completed\n",
      "reward_0 = 39.980000000000004\n",
      "reward_1 = 22.04\n",
      "episode19949 completed\n",
      "reward_0 = 11.339999999999996\n",
      "reward_1 = 18.349999999999998\n",
      "episode19999 completed\n",
      "reward_0 = 39.30000000000001\n",
      "reward_1 = 18.109999999999996\n"
     ]
    }
   ],
   "source": [
    "dqnlearning.learning(agents_CNN2, 20000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5fdc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1249, 1299, 1349, 1399, 1449, 1499, 1549, 1599, 1649, 1699, 1749, 1799, 1849, 1899, 1949, 1999, 2049, 2099, 2149, 2199, 2249, 2299, 2349, 2399, 2449, 2499, 2549, 2599, 2649, 2699, 2749, 2799, 2849, 2899, 2949, 2999, 3049, 3099, 3149, 3199, 3249, 3299, 3349, 3399, 3449, 3499, 3549, 3599, 3649, 3699, 3749, 3799, 3849, 3899, 3949, 3999, 4049, 4099, 4149, 4199, 4249, 4299, 4349, 4399, 4449, 4499, 4549, 4599, 4649, 4699, 4749, 4799, 4849, 4899, 4949, 4999, 5049, 5099, 5149, 5199, 5249, 5299, 5349, 5399, 5449, 5499, 5549, 5599, 5649, 5699, 5749, 5799, 5849, 5899, 5949, 5999, 6049, 6099, 6149, 6199, 6249, 6299, 6349, 6399, 6449, 6499, 6549, 6599, 6649, 6699, 6749, 6799, 6849, 6899, 6949, 6999, 7049, 7099, 7149, 7199, 7249, 7299, 7349, 7399, 7449, 7499, 7549, 7599, 7649, 7699, 7749, 7799, 7849, 7899, 7949, 7999, 8049, 8099, 8149, 8199, 8249, 8299, 8349, 8399, 8449, 8499, 8549, 8599, 8649, 8699, 8749, 8799, 8849, 8899, 8949, 8999, 9049, 9099, 9149, 9199, 9249, 9299, 9349, 9399, 9449, 9499, 9549, 9599, 9649, 9699, 9749, 9799, 9849, 9899, 9949, 9999, 10049, 10099, 10149, 10199, 10249, 10299, 10349, 10399, 10449, 10499, 10549, 10599, 10649, 10699, 10749, 10799, 10849, 10899, 10949, 10999, 11049, 11099, 11149, 11199, 11249, 11299, 11349, 11399, 11449, 11499, 11549, 11599, 11649, 11699, 11749, 11799, 11849, 11899, 11949, 11999, 12049, 12099, 12149, 12199, 12249, 12299, 12349, 12399, 12449, 12499, 12549, 12599, 12649, 12699, 12749, 12799, 12849, 12899, 12949, 12999, 13049, 13099, 13149, 13199, 13249, 13299, 13349, 13399, 13449, 13499, 13549, 13599, 13649, 13699, 13749, 13799, 13849, 13899, 13949, 13999, 14049, 14099, 14149, 14199, 14249, 14299, 14349, 14399, 14449, 14499, 14549, 14599, 14649, 14699, 14749, 14799, 14849, 14899, 14949, 14999, 15049, 15099, 15149, 15199, 15249, 15299, 15349, 15399, 15449, 15499, 15549, 15599, 15649, 15699, 15749, 15799, 15849, 15899, 15949, 15999, 16049, 16099, 16149, 16199, 16249, 16299, 16349, 16399, 16449, 16499, 16549, 16599, 16649, 16699, 16749, 16799, 16849, 16899, 16949, 16999, 17049, 17099, 17149, 17199, 17249, 17299, 17349, 17399, 17449, 17499, 17549, 17599, 17649, 17699, 17749, 17799, 17849, 17899, 17949, 17999, 18049, 18099, 18149, 18199, 18249, 18299, 18349, 18399, 18449, 18499, 18549, 18599, 18649, 18699, 18749, 18799, 18849, 18899, 18949, 18999, 19049, 19099, 19149, 19199, 19249, 19299, 19349, 19399, 19449, 19499, 19549, 19599, 19649, 19699, 19749, 19799, 19849, 19899, 19949, 19999]\n",
      "[2.649999999999997, -15.659999999999991, 23.749999999999993, 1.9499999999999995, -13.43, 6.599999999999996, 11.850000000000001, 15.860000000000005, -3.1400000000000032, -3.9599999999999995, 15.600000000000009, 0.13000000000000023, 1.109999999999998, 7.929999999999999, 8.589999999999998, -4.360000000000003, -3.3800000000000017, 14.910000000000004, 3.07, 5.709999999999997, 12.519999999999996, 8.15, 17.399999999999995, 3.7299999999999978, 7.360000000000001, 5.9399999999999995, 2.9599999999999973, 20.209999999999994, -1.350000000000003, -0.5900000000000006, 2.9899999999999975, -4.35, 9.14, -2.210000000000001, 4.009999999999996, 5.35, 13.21, -15.510000000000002, 10.789999999999997, 7.720000000000001, 10.390000000000002, 21.160000000000004, 16.169999999999998, 3.1899999999999995, 10.129999999999997, 6.119999999999998, 7.679999999999997, 11.590000000000005, 10.059999999999997, 7.659999999999998, 22.430000000000003, 12.060000000000004, 11.200000000000001, 13.859999999999998, 21.330000000000002, 12.36, 5.009999999999998, 9.829999999999998, 19.31, 22.360000000000014, 1.5399999999999985, 15.270000000000005, 13.920000000000002, 23.08, 21.18, 5.160000000000003, 19.22, 18.53, 6.839999999999998, 22.35, 12.75, 9.469999999999999, 13.559999999999999, 0.20999999999999752, 28.889999999999993, 4.179999999999999, 3.2399999999999975, 17.25, -1.83, 14.05, 22.55999999999999, 31.330000000000005, 26.01, 10.549999999999999, 16.890000000000004, 12.91, -10.780000000000001, 12.509999999999996, 22.090000000000007, 11.680000000000005, 3.8599999999999994, 19.98, 12.58, 4.839999999999995, 17.989999999999995, 13.929999999999994, 7.399999999999999, 3.999999999999999, 18.659999999999997, 18.840000000000003, 30.370000000000005, 18.51, 14.18, 26.15, 33.11000000000001, 26.869999999999997, 25.17, 22.91, 23.839999999999996, 14.029999999999998, 27.19, 17.470000000000002, 30.019999999999992, 9.990000000000006, 8.02, 21.71, 18.7, 13.929999999999994, 21.939999999999994, 23.91, 13.539999999999997, 28.6, 16.7, 26.479999999999993, 23.490000000000002, 24.489999999999995, 24.920000000000005, 26.669999999999995, 26.379999999999992, 26.1, 16.829999999999988, 25.419999999999998, 30.13, 17.28000000000001, 23.110000000000014, 20.30999999999999, 35.16, 19.81000000000001, 23.91, 29.789999999999996, 21.68, 28.540000000000003, 41.98, 30.260000000000005, 26.86, 35.99, 18.11, 40.10999999999999, 33.730000000000004, 19.779999999999998, 42.870000000000005, 19.37000000000001, 32.2, 30.019999999999996, 35.91999999999999, 28.55, 40.10999999999998, 25.03, 37.87, 25.059999999999995, 33.16, 27.25, 25.870000000000005, 37.56999999999999, 29.710000000000004, 34.400000000000006, 34.65999999999999, 38.22, 30.690000000000005, 29.470000000000002, 34.53999999999999, 43.58999999999999, 22.380000000000006, 26.380000000000003, 38.14, 36.18, 26.400000000000006, 40.56999999999998, 14.700000000000003, 42.29999999999998, 14.399999999999999, 37.040000000000006, 22.98, 33.620000000000005, 38.12999999999999, 23.38, 25.83000000000001, 36.48999999999999, 15.120000000000005, 21.969999999999995, 27.470000000000013, 29.440000000000005, 17.13, 41.23999999999999, 23.750000000000004, 29.669999999999998, 40.75, 24.489999999999995, 20.329999999999995, 36.88000000000001, 25.300000000000004, 42.64, 23.4, 24.909999999999997, 19.18, 36.74999999999999, 27.779999999999998, 31.43000000000002, 28.439999999999994, 39.66, 19.53, 13.299999999999997, 34.53000000000001, 30.93, 25.190000000000012, 31.429999999999996, 16.470000000000002, 33.81, 22.250000000000004, 25.329999999999995, 28.250000000000007, 40.080000000000005, 26.459999999999997, 34.269999999999996, 34.23999999999999, 39.569999999999986, 25.820000000000007, 28.560000000000006, 23.91, 33.60999999999998, 25.500000000000018, 28.57999999999999, 41.73, 34.25, 39.489999999999995, 33.76, 36.879999999999995, 37.34, 27.169999999999998, 34.38999999999999, 37.050000000000004, 30.210000000000008, 38.82, 35.97, 36.42000000000001, 26.05, 36.4, 37.099999999999994, 40.61999999999999, 46.969999999999985, 39.969999999999985, 29.860000000000003, 40.71999999999999, 19.54999999999999, 36.35999999999999, 40.46, 26.720000000000006, 29.979999999999997, 28.290000000000006, 43.699999999999996, 36.239999999999995, 27.140000000000004, 26.85, 44.09999999999999, 37.40999999999998, 38.440000000000005, 23.599999999999998, 32.690000000000005, 37.84, 23.58, 39.209999999999994, 33.81, 27.220000000000002, 28.820000000000007, 21.51, 43.51, 38.449999999999996, 48.909999999999975, 33.8, 22.57, 23.97, 42.209999999999994, 45.06999999999999, 27.149999999999995, 36.33, 26.739999999999995, 36.93999999999998, 42.36000000000001, 34.300000000000004, 28.580000000000002, 28.880000000000013, 39.72, 30.970000000000002, 24.62, 45.64999999999997, 27.860000000000003, 25.390000000000008, 29.279999999999998, 31.000000000000007, 27.64, 30.55, 36.06999999999999, 34.18000000000001, 32.750000000000014, 19.319999999999997, 39.829999999999984, 39.19999999999999, 34.3, 20.210000000000004, 36.24000000000001, 34.79, 33.58, 39.79, 29.520000000000007, 31.820000000000007, 38.53999999999999, 34.42999999999999, 24.060000000000002, 40.31999999999999, 31.569999999999986, 51.27999999999997, 37.86999999999998, 36.66999999999999, 39.18999999999998, 43.62999999999999, 35.02999999999999, 36.81, 50.010000000000005, 22.21, 28.14, 26.630000000000003, 34.26, 31.07, 27.550000000000008, 33.35999999999999, 23.98999999999999, 30.999999999999996, 25.94, 35.830000000000005, 26.140000000000004, 25.829999999999995, 14.830000000000004, 27.680000000000003, 19.35, 27.820000000000004, 43.63999999999999, 42.35, 22.300000000000004, 10.27, 26.910000000000004, 24.710000000000004, 22.010000000000005, 38.35, 16.7, 39.3, 24.35, 27.740000000000006, 29.7, 22.71000000000001, 9.809999999999999, 31.09999999999999, 35.59, 16.970000000000006, 29.14, 38.66, 46.919999999999995, 21.890000000000008, 20.11, 29.620000000000005, 28.59000000000001, 37.28, 9.41, 45.06, 39.980000000000004, 11.339999999999996, 39.30000000000001]\n",
      "[-14.18, 2.9199999999999973, -8.97, -0.08000000000000096, -0.0900000000000023, -0.5100000000000005, -5.050000000000001, -12.23, -10.049999999999999, -6.19, 8.84, -8.84, -6.679999999999999, 0.6399999999999997, -8.490000000000002, 13.519999999999994, 5.219999999999995, 8.629999999999997, -4.02, -1.8500000000000003, 14.699999999999998, 4.679999999999997, -9.79, -8.8, -2.6700000000000017, 5.069999999999996, 9.829999999999998, 23.820000000000007, 12.869999999999997, 9.270000000000001, 9.229999999999997, 7.569999999999997, 3.629999999999998, 22.830000000000002, 17.779999999999998, -2.919999999999999, 10.090000000000002, 7.919999999999998, 1.8099999999999996, -0.6100000000000023, 7.609999999999999, -2.140000000000002, 3.129999999999998, 10.630000000000003, 22.379999999999995, 9.139999999999999, 13.42, 33.190000000000005, 8.47, 3.6399999999999997, 15.949999999999996, 23.62, 11.46, 11.599999999999998, 10.98, 12.989999999999993, 5.739999999999999, 7.299999999999999, 2.1899999999999995, 13.370000000000001, 9.840000000000002, 28.24000000000001, 0.7799999999999991, 7.559999999999999, 25.569999999999997, 8.629999999999995, 8.229999999999999, 18.510000000000005, 24.9, 3.989999999999997, 15.789999999999997, 3.1099999999999977, 21.880000000000003, 24.25, 6.539999999999998, 22.71, 8.53, 15.79, 37.81999999999999, 18.000000000000004, 8.889999999999997, 13.55999999999999, 11.149999999999999, 9.540000000000003, 13.459999999999997, 4.5399999999999965, 9.349999999999998, 5.209999999999996, 31.640000000000008, 12.510000000000002, 9.759999999999998, 9.229999999999999, 20.680000000000003, 1.639999999999997, 8.599999999999998, 24.290000000000003, 5.139999999999999, 13.920000000000002, 20.299999999999997, 24.979999999999997, 22.179999999999993, 5.509999999999997, 15.389999999999997, 20.330000000000002, 20.159999999999993, 11.499999999999996, 14.45, 12.869999999999996, 7.579999999999997, 10.249999999999993, 25.769999999999992, 16.679999999999996, 23.88, 25.390000000000008, 18.2, 17.630000000000003, 1.5599999999999987, 23.680000000000003, 4.819999999999999, 16.639999999999997, 20.22, 7.030000000000001, 6.939999999999996, 17.530000000000005, 19.070000000000007, 11.569999999999999, 18.709999999999997, 6.430000000000001, 14.470000000000002, 18.330000000000002, 22.210000000000008, 4.4099999999999975, 22.15, 19.19, 15.490000000000002, 20.29999999999999, 8.79, 21.43, 13.77, 22.269999999999996, 15.589999999999995, 36.11, 14.650000000000007, 28.580000000000002, 39.530000000000015, 12.359999999999994, 18.409999999999993, 14.950000000000001, 19.9, 10.72, 31.509999999999998, 31.2, 3.2399999999999984, 22.559999999999995, 13.559999999999999, 20.790000000000006, 16.619999999999997, 11.319999999999993, 13.980000000000004, 12.74, 32.64000000000001, 31.590000000000003, 16.47, 42.55999999999998, 11.540000000000001, 8.89, 28.75000000000001, 34.39, 9.429999999999996, 31.349999999999994, 33.19, 33.16000000000001, 4.939999999999998, 27.83000000000001, 20.030000000000005, 7.080000000000001, 10.920000000000002, 25.79, 31.320000000000014, 26.709999999999994, 8.780000000000003, 28.840000000000003, 18.9, 25.31, 3.5299999999999985, 15.240000000000002, 17.58, 16.450000000000003, 9.009999999999996, 17.639999999999997, 15.830000000000002, 17.169999999999998, 28.019999999999996, 32.82, 24.400000000000006, 29.12, 12.69, 28.15000000000001, 19.740000000000002, 25.69000000000001, 18.440000000000005, 37.66, 23.34, 26.409999999999997, 16.65, 32.29, 24.78000000000001, 25.309999999999995, 13.360000000000005, 27.179999999999996, 25.2, 36.059999999999995, 25.68, 24.810000000000006, 35.88000000000001, 15.430000000000003, 28.109999999999996, 23.840000000000007, 11.759999999999998, 19.480000000000004, 12.240000000000002, 32.56999999999999, 26.610000000000003, 11.379999999999999, 27.080000000000002, 26.379999999999995, 10.729999999999999, 15.210000000000004, 16.690000000000005, 32.28000000000001, 33.26, 8.769999999999998, 9.149999999999997, 22.150000000000013, 25.169999999999998, 26.320000000000007, 24.480000000000004, 15.420000000000002, 13.240000000000002, 27.14, 20.019999999999996, 32.06999999999999, 21.680000000000007, 22.7, 5.6299999999999955, 19.429999999999996, 18.800000000000004, 29.72, 20.390000000000008, 28.750000000000004, 21.98, 26.93, 13.879999999999999, 21.829999999999995, 14.810000000000002, 18.28, 32.74999999999999, 29.26000000000001, 14.519999999999996, 21.390000000000008, 36.08000000000001, 17.280000000000005, 8.839999999999998, 17.360000000000007, 20.610000000000003, 5.189999999999997, 28.660000000000004, 16.96, 28.33, 40.760000000000005, 28.929999999999996, 14.7, 23.009999999999998, 26.37999999999999, 39.95000000000001, 21.630000000000006, 21.339999999999996, 14.999999999999998, 18.13, 15.91, 33.959999999999994, 17.480000000000004, 36.64999999999999, 22.57999999999999, 30.78, 22.229999999999997, 31.39, 28.96, 19.659999999999997, 17.58000000000001, 30.609999999999996, 32.410000000000004, 26.930000000000003, 15.869999999999997, 25.94, 29.82, 19.739999999999995, 24.65999999999999, 22.78, 22.28, 21.1, 22.060000000000006, 16.709999999999997, 22.530000000000005, 32.050000000000004, 38.42999999999999, 11.360000000000001, 11.129999999999999, 24.02, 26.929999999999996, 37.25, 19.149999999999995, 21.810000000000002, 33.99, 32.6, 27.48, 30.75, 38.72, 16.000000000000004, 19.95, 31.820000000000007, 26.30000000000001, 23.819999999999997, 33.230000000000004, 28.16, 33.510000000000005, 26.670000000000005, 18.230000000000004, 28.899999999999995, 31.13, 36.08999999999998, 31.08000000000001, 20.62999999999999, 25.25, 39.019999999999996, 31.530000000000012, 26.950000000000006, 18.910000000000004, 18.380000000000006, 38.07, 25.26000000000001, 33.55, 16.75, 37.070000000000014, 36.85, 21.420000000000005, 27.560000000000006, 31.5, 13.889999999999997, 29.18000000000001, 27.070000000000004, 18.470000000000006, 23.91, 19.410000000000004, 38.25000000000001, 37.74000000000001, 22.77, 34.49999999999999, 40.35, 11.590000000000002, 31.719999999999988, 32.220000000000006, 32.28999999999999, 35.64000000000001, 29.04999999999999, 39.59999999999999, 31.34, 27.690000000000005, 17.920000000000005, 18.780000000000005, 28.950000000000003, 26.219999999999995, 33.190000000000005, 22.04, 18.349999999999998, 18.109999999999996]\n"
     ]
    }
   ],
   "source": [
    "print(dqnlearning.episode_buffer)\n",
    "print(dqnlearning.rewards_0)\n",
    "print(dqnlearning.rewards_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5af8ff75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqnlearning.episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d9f46e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0574e-01,  1.1523e-01, -1.0000e+09, -1.0000e+09,  1.5073e-01,\n",
       "          8.6536e-02, -1.0000e+09, -1.0000e+09,  1.3279e-01,  1.1295e-01,\n",
       "         -1.0000e+09, -1.0000e+09,  9.3971e-02,  8.6145e-02, -1.0000e+09,\n",
       "         -1.0000e+09,  1.3290e-01,  1.3460e-01, -1.0000e+09, -1.0000e+09,\n",
       "          1.1042e-01,  6.1178e-02, -1.0000e+09, -1.0000e+09,  1.4451e-01,\n",
       "          1.1740e-01, -1.0000e+09, -1.0000e+09,  1.1219e-01,  7.2284e-02,\n",
       "         -1.0000e+09, -1.0000e+09,  1.2025e-01,  1.1254e-01, -1.0000e+09,\n",
       "         -1.0000e+09]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = [[8, 2, 5], [4, 6, None], [4, 2, None]]\n",
    "board = make_board_tensor(board)\n",
    "hands = torch.tensor([2,1,1], dtype=torch.float32)\n",
    "agents_CNN2[0].qnet(board.unsqueeze(0), hands.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c8b53",
   "metadata": {},
   "source": [
    "DQN学習　Agent vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "# --- 学習後の評価（探索オフで100局） ---\n",
    "def eval_vs_random(agent: DQNAgent, n_games=100):\n",
    "    wins_as_first = 0\n",
    "    wins_as_second = 0\n",
    "    for _ in range(n_games):\n",
    "        # 先手\n",
    "        s = start_game([\"agent\", \"random\"], rules, mission_id=[0,1])\n",
    "        done = False\n",
    "        while not done:\n",
    "            pid = s.turn\n",
    "            if pid == 0:\n",
    "                aid = agent.get_action(s, pid, False)  # 探索なし\n",
    "                x, y, num_index = id_to_action(board_size, s.hands[pid], aid)\n",
    "                make_move(s, pid, x, y, num_index, \"add\")\n",
    "            else:\n",
    "                aid, (x, y, num_index) = random_legal_action(s, pid)\n",
    "                make_move(s, pid, x, y, num_index, \"add\")\n",
    "            done, rew = get_reward(s)\n",
    "        wins_as_first += (rew[0] > 0)\n",
    "\n",
    "        # 後手\n",
    "        s = start_game([\"random\", \"agent\"], rules, mission_id=[0,1])\n",
    "        done = False\n",
    "        while not done:\n",
    "            pid = s.turn\n",
    "            if pid == 1:\n",
    "                aid = agent.get_action(s, pid, False)\n",
    "                x, y, num_index = id_to_action(board_size, s.hands[pid], aid)\n",
    "                make_move(s, pid, x, y, num_index, \"add\")\n",
    "            else:\n",
    "                aid, (x, y, num_index) = random_legal_action(s, pid)\n",
    "                make_move(s, pid, x, y, num_index, \"add\")\n",
    "            done, rew = get_reward(s)\n",
    "        wins_as_second += (rew[1] > 0)\n",
    "    print(f\"eval: first-win {wins_as_first}/{n_games}, second-win {wins_as_second}/{n_games}\")\n",
    "\n",
    "\n",
    "def random_legal_action(s, pid):\n",
    "    \"\"\"適当にサンプリングして合法手が見つかるまで回す（簡易版）\"\"\"\n",
    "    while True:\n",
    "        aid = random.randrange(ACTION_SPACE)\n",
    "        x, y, num_index = id_to_action(board_size, s.hands[pid], aid)\n",
    "        # is_valid_move を使えるならこちらが安全\n",
    "        try:\n",
    "            if is_valid_move(s, pid, x, y, num_index, \"add\"):\n",
    "                return aid, (x, y, num_index)\n",
    "        except NameError:\n",
    "            # is_valid_move が無い場合は make_move を試してロールバック…は重いのでスキップ\n",
    "            return aid, (x, y, num_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a21859fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_xyi_simple(s:GameState, pid):\n",
    "    \"\"\"\n",
    "    形式:  x,y,idx   例: 1,2,0\n",
    "    - 0始まりの (x,y)\n",
    "    - idx は手札インデックス\n",
    "    常に op=\"add\" で返します。\n",
    "    \"\"\"\n",
    "    n = s.rules.board_size\n",
    "    while True:\n",
    "        raw = input(\"x,y,idx をカンマ区切りで: \")  # 例: 1,2,0\n",
    "        try:\n",
    "            x_str, y_str, idx_str = [t.strip() for t in raw.split(\",\")]\n",
    "            x, y, idx = int(x_str), int(y_str), int(idx_str)\n",
    "        except Exception:\n",
    "            print(\"形式エラー：例) 1,2,0\")\n",
    "            continue\n",
    "\n",
    "        if not (0 <= x < n and 0 <= y < n):\n",
    "            print(f\"x,y は 0..{n-1} の範囲です\")\n",
    "            continue\n",
    "        if not (0 <= idx < len(s.hands[pid])):\n",
    "            print(f\"idx は 0..{len(s.hands[pid])-1} の範囲です\")\n",
    "            continue\n",
    "        return x, y, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf8357a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None, None, None], [None, None, None], [None, None, None]]\n",
      "[4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "s = start_game([\"agent_0\",\"player\"],Rules(),mission_id=[0,1])\n",
    "done = False\n",
    "pid = s.turn\n",
    "print(s.board)\n",
    "print(s.hands[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bd33f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2, 1)\n",
      "[[None, None, None], [None, None, None], [4, None, None]]\n",
      "[4, 4, 4]\n",
      "1\n",
      "形式エラー：例) 1,2,0\n",
      "[[4, None, None], [None, None, None], [4, None, None]]\n",
      "[4, 4, 2]\n",
      "0\n",
      "(2, 0, 1)\n",
      "[[4, None, 4], [None, None, None], [4, None, None]]\n",
      "[4, 4, 2]\n",
      "1\n",
      "[[4, None, 4], [4, None, None], [4, None, None]]\n",
      "[4, 2, 1]\n",
      "0\n",
      "(1, 0, 0)\n",
      "[[4, 1, 4], [4, None, None], [4, None, None]]\n",
      "[4, 2, 1]\n",
      "1\n",
      "[[8, 1, 4], [4, None, None], [4, None, None]]\n",
      "[2, 1, 4]\n",
      "0\n",
      "(1, 0, 2)\n",
      "[[8, 2, 4], [4, None, None], [4, None, None]]\n",
      "[2, 1, 4]\n",
      "1\n",
      "[[8, 2, 4], [4, 4, None], [4, None, None]]\n",
      "[2, 1, 1]\n",
      "0\n",
      "(2, 0, 2)\n",
      "[[8, 2, 5], [4, 4, None], [4, None, None]]\n",
      "[2, 1, 1]\n",
      "1\n",
      "[[8, 2, 5], [4, 4, None], [4, 2, None]]\n",
      "[1, 1, 2]\n",
      "0\n",
      "(1, 1, 2)\n",
      "[[8, 2, 5], [4, 6, None], [4, 2, None]]\n",
      "[1, 1, 2]\n",
      "1\n",
      "[[8, 2, 5], [4, 6, None], [4, 3, None]]\n",
      "[1, 2, 1]\n",
      "0\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    \"\"\"盤面と手札の表示\"\"\"\n",
    "\n",
    "    if pid == 0:\n",
    "        aid = agents_CNN2[0].get_action(s,pid,False)\n",
    "        x, y, num_index = id_to_action(3, s.hands[pid], aid)\n",
    "        print(id_to_action(3, s.hands[pid], aid))\n",
    "        make_move(s, pid, x, y, num_index,\"add\")\n",
    "    \n",
    "    else:\n",
    "        \"\"\"プレイヤーの操作\"\"\"\n",
    "        x,y,num_index = input_xyi_simple(s, pid)\n",
    "        make_move(s, pid, x, y, num_index,\"add\")\n",
    "        \n",
    "    print(s.board)\n",
    "    print(s.hands[1])\n",
    "    print(s.turn)\n",
    "    pid = s.turn\n",
    "    done, _ = get_reward(s)\n",
    "    if done:\n",
    "        print(s.winners)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
